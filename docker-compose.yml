version: '3.8'

services:
  app:
    build: .
    ports:
      - "5000:8000"
    volumes:
      - .:/app
      - cache_data:/app/cache
      - vector_store_data:/app/chroma_db
    environment:
      - OLLAMA_HOST=http://localai:8080
      - TZ=Europe/Istanbul
    restart: unless-stopped
    depends_on:
      - localai
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2
              capabilities: [gpu]
    networks:
      - ragollama_network

  localai:
    image: localai/localai:v2.7.0
    ports:
      - "8080:8080"
    volumes:
      - localai_data:/models
      - /usr/lib/ollama/cuda_v12:/usr/lib/ollama/cuda_v12
    environment:
      - CUDA_VISIBLE_DEVICES=0,1
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=all
      - DEBUG=1
      - GPU_LAYERS=9999
      - CUDA_LAUNCH_BLOCKING=1
      - CUDA_MODULE_LOADING=LAZY
      - CUDA_AUTO_BOOST=0
      - CUDA_DEVICE_MAX_CONNECTIONS=1
      - CUDA_CACHE_PATH=/tmp/cuda-cache
      - CUDA_CACHE_MAXSIZE=67108864
      - LD_LIBRARY_PATH=/usr/lib/ollama/cuda_v12:$LD_LIBRARY_PATH
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2
              capabilities: [gpu]
    restart: unless-stopped
    networks:
      - ragollama_network

networks:
  ragollama_network:
    driver: bridge

volumes:
  localai_data:
  cache_data:
  vector_store_data: 